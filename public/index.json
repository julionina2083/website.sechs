[{"categories":null,"contents":"CPS workflow   if (document.querySelector('body').getAttribute('class') != \"dark\") { document.querySelectorAll('.mermaidspan').forEach(el = { el.innerHTML = '\\ngraph TD;\\n start[START] --\\u003e A\\n subgraph Updates To Certs Initiated\\n A[Create certificate with all subdomains in Akamai CPS GUI] --\\u003e\\n B[Create Certificate Signing Request - CSR] --\\u003e\\n C[Submit CSR to LetsEncrypt]\\n end\\n\\n subgraph Manual Validation -- very time consuming!\\n C --\\u003e\\n D[List of DNS ACME Validation records returned] --\\u003e\\n E[Add each record by hand in the Akamai DNS GUI] --\\u003e\\n F[Wait an hour until LetsEncrypt validates ownership of domains] \\n end\\n\\n subgraph Akamai Cert Provisioning System \\n F --\\u003e\\n G[Retrieve certificates] --\\u003e\\n H[Push to Akamai staging] --\\u003e\\n I{Is always test\\u003cbr\\u003eon staging on?}\\n I -- yes --\\u003e test[Wait until manual validation]\\n I -- no --\\u003e deploy[Deploy to production]\\n test --\\u003e wait[Wait 60 days until certificate expiry]\\n deploy --\\u003e wait[Wait 60 days until certificate expiry]\\n end\\n wait --\\u003e return[Do it all over again]\\n\\n class D orange;\\n class E orange;\\n class F orange;\\n classDef orange fill:#f1b185, color: black;\\n' }) } else { document.querySelectorAll('.mermaidspan').forEach(el = { el.innerHTML = '\\ngraph TD;\\n start[START] --\\u003e A\\n subgraph Updates To Certs Initiated\\n A[Create certificate with all subdomains in Akamai CPS GUI] --\\u003e\\n B[Create Certificate Signing Request - CSR] --\\u003e\\n C[Submit CSR to LetsEncrypt]\\n end\\n\\n subgraph Manual Validation -- very time consuming!\\n C --\\u003e\\n D[List of DNS ACME Validation records returned] --\\u003e\\n E[Add each record by hand in the Akamai DNS GUI] --\\u003e\\n F[Wait an hour until LetsEncrypt validates ownership of domains] \\n end\\n\\n subgraph Akamai Cert Provisioning System \\n F --\\u003e\\n G[Retrieve certificates] --\\u003e\\n H[Push to Akamai staging] --\\u003e\\n I{Is always test\\u003cbr\\u003eon staging on?}\\n I -- yes --\\u003e test[Wait until manual validation]\\n I -- no --\\u003e deploy[Deploy to production]\\n test --\\u003e wait[Wait 60 days until certificate expiry]\\n deploy --\\u003e wait[Wait 60 days until certificate expiry]\\n end\\n wait --\\u003e return[Do it all over again]\\n\\n class D orange;\\n class E orange;\\n class F orange;\\n classDef orange fill:#f1b185, color: black;\\n' }) }   Demo     ACME Validator Demo     if (document.querySelector('body').getAttribute('class') != \"dark\") { document.querySelectorAll('.asciinema-player').forEach(el = { el.innerHTML = '' }) } else { document.querySelectorAll('.asciinema-player').forEach(el = { el.innerHTML = '' }) }   window.addEventListener('DOMContentLoaded', function() { document.querySelector('#id-2  .window-header  .window-header-btns  .window-header__btn--close').onclick = () = { document.querySelector('#id-2').remove() } document.querySelector('#id-2  .window-header  .window-header-btns  .window-header__btn--maximize').onclick = () = { document.querySelector('#id-2 * .fullscreen-button').click() } document.querySelector('#id-2  .window-header  .window-header-btns  .window-header__btn--minimize').onclick = () = { minBtn = document.querySelector('#id-2 * .asciinema-player') if (minBtn.style.display === \"none\") { minBtn.style.display = \"block\" } else { minBtn.style.display = \"none\" } } })  Code #!/bin/bash # automates ACME validation for Akamai # 1. fetch all pending CPS ACME validation DNS records # 2. fetch all zone files  # 3. update zone files with new ACME validation TXT records  # 4. upload the new zone files  set -e  # fetching all CNs with pending cert changes\u0026#34; CNs=$(akamai cps list | grep \u0026#39;dv san\u0026#39; | grep \u0026#39;*Yes*\u0026#39; | cut -f3 -d\u0026#39;|\u0026#39; | awk \u0026#39;{print $1}\u0026#39;)   # fetch all ACME validation records for each Domain in Akamai rawRecords=\u0026#34;\u0026#34; for CN in ${CNs}; do  rawRecords=$(echo -e \u0026#34;$rawRecords\\\\n$(akamai cps status --cn \u0026#34;$CN\u0026#34; --validation-type dns 2\u0026gt;\u0026amp;1 | grep Awaiting)\u0026#34;) done   # fetch all zones\u0026#34; zones=$(akamai dns list-zoneconfig --summary | grep ACTIVE | awk \u0026#39;{print $1}\u0026#39;)  # clean up files from our previous run  [[ -e \u0026#34;./zonefiles\u0026#34; ]] \u0026amp;\u0026amp; rm -rf \u0026#34;./zonefiles\u0026#34; mkdir zonefiles  for zone in ${zones}; do   # skip over zones which don\u0026#39;t have any pending changes  [[ $(echo \u0026#34;$rawRecords\u0026#34; | grep $zone) ]] || continue   # fetch zone file for each zone  akamai dns retrieve-zoneconfig $zone -dns --output \u0026#34;./zonefiles/${zone}.zone.tmp2\u0026#34;   # increment SOA serial for each zone file   awk \u0026#39;BEGIN{ OFS=\u0026#34;\\t\u0026#34; } /SOA/{$7=$7+1} 1\u0026#39; \u0026#34;./zonefiles/${zone}.zone.tmp2\u0026#34; \u0026gt; \u0026#34;./zonefiles/${zone}.zone.tmp\u0026#34;   # delete old acme records  grep -v \u0026#34;_acme-challenge.\u0026#34; \u0026#34;./zonefiles/${zone}.zone.tmp\u0026#34; \u0026gt; \u0026#34;./zonefiles/${zone}.zone\u0026#34;   # append new acme records for $zone ${NC}\u0026#34;  echo \u0026#34;$rawRecords\u0026#34; \\  | grep $zone \\  | awk \u0026#39;{print \u0026#34;_acme-challenge.\u0026#34; $2 \u0026#34;.\\t\u0026#34; \u0026#34;60\\t\u0026#34; \u0026#34;IN\\t\u0026#34; \u0026#34;TXT\\t\u0026#34; $7}\u0026#39; \\  \u0026gt;\u0026gt; \u0026#34;./zonefiles/${zone}.zone\u0026#34;    # upload our edited zonefile  akamai dns update-zoneconfig $zone -dns -file ./zonefiles/${zone}.zone  done  exit 0 ","date":"05","image":null,"permalink":"https://deoxy.net/blog/automating-akamai/","tags":["akamai","letencrypt","acme","dns","https","bash"],"title":"Automating Akamai / LetsEncrypt Cert Validation"},{"categories":null,"contents":"What\u0026rsquo;s a CSP? One of the mitigating defenses for XSS attacks and Clickjacking attacks is a good Content Security Policy (CSP). While not a pancea, it can effectively limit the severity of any exploits by constraining the XSS payload size to the injection window, which is typically limited to a few characters. Instead of externally loading a payload like:\n\u0026lt;script src=\u0026#34;https://evil.com/payload.js\u0026#34;/\u0026gt; the entire payload must be encoded in the script evaluation window, effectively preventing nasty frameworks like BeEF from being loaded.\n  if (document.querySelector('body').getAttribute('class') != \"dark\") { document.querySelectorAll('.mermaidspan').forEach(el = { el.innerHTML = '\\nsequenceDiagram\\n Browser-\\u003e\\u003e\\u002bServer: REQUEST https:\\/\\/example.com\\/assets\\/js\\/lib.js \\n Server--\\u003e\\u003e\\u002bBrowser: 200 OK\\n Browser-\\u003e\\u003e\\u002bServer: REQUEST https:\\/\\/example.com\\/static\\/img\\/kitty.jpg\\n Server--\\u003e\\u003e\\u002bBrowser: 200 OK\\n Note over Server,Browser: Content-Security-Policy:\\u003cbr\\u003edefault-src https:\\/\\/example.com \\n Browser-\\u003e\\u003e\\u002bServer: REQUEST https:\\/\\/evil.org\\/xss.js\\n Server--\\u003e\\u003e-Browser: 400 Not Allowed (blocked:csp)\\n' }) } else { document.querySelectorAll('.mermaidspan').forEach(el = { el.innerHTML = '\\nsequenceDiagram\\n Browser-\\u003e\\u003e\\u002bServer: REQUEST https:\\/\\/example.com\\/assets\\/js\\/lib.js \\n Server--\\u003e\\u003e\\u002bBrowser: 200 OK\\n Browser-\\u003e\\u003e\\u002bServer: REQUEST https:\\/\\/example.com\\/static\\/img\\/kitty.jpg\\n Server--\\u003e\\u003e\\u002bBrowser: 200 OK\\n Note over Server,Browser: Content-Security-Policy:\\u003cbr\\u003edefault-src https:\\/\\/example.com \\n Browser-\\u003e\\u003e\\u002bServer: REQUEST https:\\/\\/evil.org\\/xss.js\\n Server--\\u003e\\u003e-Browser: 400 Not Allowed (blocked:csp)\\n' }) }   CSP\u0026rsquo;s work by essentially \u0026ldquo;whitelisting\u0026rdquo; externally loaded content. If evil.com is not whitelisted for loading scripts, scripts from evil.com cannot be loaded into the site. Sounds great, right? Unfortunately, the reality is that CSPs are only enforced on 7% of the Alexa Top 1M sites.\nWhy is this? If you\u0026rsquo;ve ever tried implementing a CSP on a non-trivial site, you\u0026rsquo;ll know the number one difficulty is breaking the site by preventing legitimate content from being loaded\u0026ndash; oftentime on pages you never expected to have content on. It\u0026rsquo;s no wonder top site owners are slow to implement CSPs.\nGiven an existing sites with tons of legacy content, how does one go about finding the specific external sources for each CSP directive? One answer could be opening up devtools and browsing a few pages of the site, then writing it by hand. This might be fine for a single tiny site. But what if you have an entire company\u0026rsquo;s worth of large sites to handle?\n TLDR: Use my Playwright script to generate comprehensive CSP\u0026rsquo;s quickly.\n Content Security Policy Generator (Chrome Extension) Luckily for us, there is a chrome extension called Content Security Policy (CSP) Generator which will help us generate a CSP on all visited links.\nHowever, we still need to visit all the links. You could click them all manually, but that would also take you ages. Besides, don\u0026rsquo;t you have more important things to do, such as sitting in meetings? Browser automation to the rescue!\nPlaywright I\u0026rsquo;ll be using Playwright, which is typically used to control a full browser via the Chrome DevTools protocol for QA testing purposes. It\u0026rsquo;s a fork of Puppeeteer. I prefer to use Playwright, due to it\u0026rsquo;s more user-friendly selector engine.\nThe advantage of using a full browser to crawl our site is that all dynamic content will be loaded. In this day and age, almost all sites are built using some javascript framework, which means a full browser is necessary to load all content for the Chrome extension to evaluate, leading to a more airtight CSP.\nWriting the script Downloading the chrome extension Instead of downloading the chrome extension by going to the web store, we will use curl to download the .crx source file. This is so we can load our plugin into playwright after we unzip it.\nWe will also use a free CORS reverse proxy called CORS Anywhere, in order to bypass the Chrome webstore\u0026rsquo;s security policy. If we don\u0026rsquo;t do this, the resulting response body will be empty.\nextension_id=\u0026#34;ahlnecfloencbkpfnpljbojmjkfgnmdc\u0026#34;  curl -s -L -o \u0026#34;./csper.zip\u0026#34; \u0026#34;https://cors-anywhere.herokuapp.com/https://clients2.google.com/service/update2/crx?response=redirect\u0026amp;os=win\u0026amp;arch=x86-64\u0026amp;os_arch=x86-64\u0026amp;nacl_arch=x86-64\u0026amp;prod=chromiumcrx\u0026amp;prodchannel=unknown\u0026amp;prodversion=9999.0.9999.0\u0026amp;acceptformat=crx2,crx3\u0026amp;x=id%3D${extension_id}%26uc\u0026#34; \\  -H \u0026#39;Origin: https://robwu.nl\u0026#39;\\  -H \u0026#39;Referer: https://robwu.nl/\u0026#39;\\  unzip -d \u0026#34;csper-src\u0026#34; \u0026#34;csper.zip\u0026#34; Loading the CSPer extension into Playwright Here, we provide some additional args to load the extension directory we just downloaded into Chrome. Note that we need to turn off headess mode for extensions to work.\n(async () =\u0026gt; {  const pathToExtension = require(\u0026#39;path\u0026#39;).join(__dirname, \u0026#39;csper-src\u0026#39;);  const userDataDir = \u0026#39;./user-data-dir\u0026#39;;  const browserContext = await chromium.launchPersistentContext(userDataDir,{  headless: false,  args: [  `--disable-extensions-except=${pathToExtension}`,  `--load-extension=${pathToExtension}`  ]  });   // get url to load from command line  cspUrl = process.argv.slice(2)[0]   const page = await browserContext.newPage();  await page.goto(cspUrl) })() Crawling each page recursively In our playwright file, we define a function crawl() which will take a URL, scrape all \u0026lt;a href=\u0026quot;\u0026quot;/\u0026gt; links off the page. This function uses another function called waitForNetworkSettled(), taken from this gist. It\u0026rsquo;s basically an alternative to page.waitForNavigation({ waitUntil: \u0026quot;networkidle\u0026quot;})), which waits until the page loads. In my experience, the native function is buggy and resolves too early, so I had to use an alternative.\n const seenURLs = new Set()  const crawl = async (url) =\u0026gt; { \t// don\u0026#39;t recrawl pages alrady visited  if (seenURLs.has(url)) {  return  }  \t// only crawl pages that are within our base domain  seenURLs.add(url)  if (!url.startsWith(cspUrl)) {  return  }  \t// don\u0026#39;t crawl documents  if (url.endsWith(\u0026#34;.pdf\u0026#34;) || url.endsWith(\u0026#34;.docx\u0026#34;) || url.endsWith(\u0026#34;.xlsx\u0026#34;)) {  return  }  \t// define a request function that will wait until the page is loaded \t// will also scroll down to handle lazy-loaded items  const doRequest = waitForNetworkSettled(page, async () =\u0026gt; {  await page.goto(url, { waitutil: \u0026#39;domcontentloaded\u0026#39; })  await page.evaluate(() =\u0026gt; window.scrollTo(0, (document.body.scrollHeight/3)));  })  \t// race the request function with a timeout function \t// this will allow page that don\u0026#39;t stop loading assets to continue  await Promise.race([  doRequest,  new Promise((_, reject) =\u0026gt; setTimeout(() =\u0026gt; reject(new Error(\u0026#39;timeout\u0026#39;)), 11.5e3))  ]).catch()   try { \t// scrape new URLS off the page  const urls = await page.$$eval(\u0026#39;a\u0026#39;, (elements) =\u0026gt;  elements.map((el) =\u0026gt; el.href),  )  \t// recursively crawl them  for await (const u of urls) {  await crawl(u)  }  } catch {}  } Generating our CSP First, run the program.\nnode generate.js \u0026#34;https://polb.com\u0026#34; When the browser loads, click on the extension icon and start a new recording.\nThen, press enter to start recursively visiting all the URLs with the script.\n  Be mindful of being banned if there is an anti-bot service running on the site. Set a delay between page loads if necessary.\nWhen it is done, generate your new CSP policy.\nDon\u0026rsquo;t forget to remove any inline scripts from your site.\nDeploying the CSP Voila! you are done. Now go deploy your CSP by adding the\nContent-Security-Policy: \u0026#34;your_generated_csp\u0026#34; header to your site, if you have control over the server.\nIf you have control over the content, but not the server, you can add this html tag:\n\u0026lt;meta http-equiv=”Content-Security-Policy” content=”\u0026lt;your_generated_csp\u0026gt;”/\u0026gt; Hopefully this saves you some time. Source code here if you\u0026rsquo;d like to use it.\n","date":"04","image":null,"permalink":"https://deoxy.net/blog/csp/","tags":["playwright","js","content security policy","appsec"],"title":"Implementing Content Security Policies, The Easy Way."}]